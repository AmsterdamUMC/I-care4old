{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "022a2832",
   "metadata": {},
   "source": [
    "# Causal Machine Learning for Treatment Effect Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9749f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#from sklearn.impute import KNNImputer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from sklearn.preprocessing import Normalizer\n",
    "import pickle\n",
    "\n",
    "# get standard models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostRegressor, AdaBoostClassifier\n",
    "from sklearn.ensemble import ExtraTreesRegressor, ExtraTreesClassifier\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPRegressor, MLPClassifier\n",
    "\n",
    "from sources.doubly_robust import doubly_robust\n",
    "from sources.models.TARNet import TARnetICFR\n",
    "\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from auxiliary import data_processing, doubly_robust, visualise, visualise_ites_proba, visualise_ites, impute_missing_values_knn, run_model, undersample, full_contra_indications_tracker, value_based_contra_indications_tracker, period_decomposition, run_model_class, multicol\n",
    "from icodes import encoding\n",
    "\n",
    "# from datetime import timedelta\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6bccc9",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "The purpose of this notebook is to provide and end-to-end description of the treatment effect estimation process. This notebook consists of three parts:\n",
    "\n",
    "1. **Data processing**: The first part is the data processing part. Here we basically prepare the data for the ML models.\n",
    "2. **Causal ML Models**: The second part of the notebook consists of a series of causal ML learning models. In particular, we use the causal meta-learning framework. \n",
    "3. **Adadptive Model**: The third part of the notebook is also a machine learning model. However, contrary to the models before, this one was developed by the VU team and allows for more control over the estimation proces. \n",
    "\n",
    "Please find below an overview of the steps we will go through. Note that the boxes do not correspond one-to-one to sections in the notebook, but the general flow is the same. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f80659",
   "metadata": {},
   "source": [
    "![alt text](images\\ModellingOverview.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3702ec1",
   "metadata": {},
   "source": [
    "## 1.a. Preliminaries \\& Design Choices\n",
    "\n",
    "We first want to get some preliminaries out of the way. In particular, we would like to make some specific **design choices** explicit. Below, we can set the following parameters, thereby modifying specific choices:\n",
    "\n",
    "- ``EXPOSURE_THRESHOLD``: The treatment is binary. However, how much treatment is provided can be changed with this variable. In essence, it determines the cut-off value. For instance, we can set it to 60 minutes in the case of physical therapy (in mins.). Then, patients with more then 60 minutes of PT will be assigned T=1, those below will be T=0.\n",
    "- ``PERIOD_MIN``: Minimal time period between baseline measurement and followup measurement. N.B. If this is set too low, the effect may not yet register. Too high and the effect may have faded out. \n",
    "- ``PERIOD_MAX``: Maximal time period between baseline measurement and followup measurement. N.B. If this is set too low, effect may not yet register. Too high and the effect may have faded out.  \n",
    "- ``REMOVE_MULTI_COL``: Whether or not to remove multicollinear columns. \n",
    "- ``CORRELATION_THRESHOLD``: Threshold for removing collinear columns. \n",
    "- ``UNDERSAMPLE``: Some models perform better when undersampling the minority class. For instance, if there are 1000 cases of patients with PT (T=0) and only 150 with PT (T=1), setting this to ``True`` ensures that 150 patients are sampled from PT (T=0), resulting in 300 observations in total.\n",
    "- ``IMPUTE``: Whether or not to impute missing *covariate* values.\n",
    "- ``PROPENSITY``: Whether or not to add a propensity score to the model.\n",
    "- ``CLEAN_COMPARE``: By default (when this is set to ``False``), the data processing script assigns any observations below ``EXPOSURE_THRESHOLD`` to the control group and  any observation above ``EXPOSURE_THRESHOLD`` to treated. Setting this parameter to ``True`` creates a scenario where *only* observation that did not get any treatment are assigned to the control group. For instance, in the case of PT, we would have a control group of patients getting only 0 mins. of PT and a treated group of patients with, say, more than 60 mins. of PT.\n",
    "- ``TREATMENT``: Name of the treatment variable.\n",
    "- ``TARGET``: Name of the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90862091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# set the exposure threshold for binary dichtomization.\n",
    "EXPOSURE_THRESHOLD = 1\n",
    "\n",
    "# set the period between observations. We only consider the effects between 60 and 120 days after exposure. \n",
    "PERIOD_MIN = 90\n",
    "PERIOD_MAX = 270\n",
    "\n",
    "# if we want to move multicollinear columns, set to True\n",
    "REMOVE_MULTI_COL = True\n",
    "\n",
    "# set the threshold for multicollinearity drops \n",
    "CORRELATION_THRESHOLD = 0.6\n",
    "\n",
    "# set to True if we want to undersample\n",
    "UNDERSAMPLE = False\n",
    "\n",
    "# set to True if we want to impute missing values\n",
    "IMPUTE = True\n",
    "\n",
    "PROPENSITY = True\n",
    "\n",
    "# clean compare (to be used if only > threshold AND treatment = 0)\n",
    "CLEAN_COMPARE = True\n",
    "\n",
    "TREATMENT = 'ig6a' # (hours of exc.)\n",
    "OUTCOME = 'in5a'  # CHANGERE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f092037",
   "metadata": {},
   "source": [
    "Having set the desired parameters, we will now read in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77ac00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Dutch_HC_v2.csv\", sep = ';')\n",
    "df.head()\n",
    "\n",
    "print(f\"Treatment before processing {len(df[df[TREATMENT]>EXPOSURE_THRESHOLD])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb66dcba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define codes for missing values\n",
    "missing_codes = [-1,-2]\n",
    "# replace missing codes with NaN\n",
    "df.replace(missing_codes, np.nan, inplace=True)\n",
    "#confirm missing values are handles\n",
    "print(df.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c900d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(f\"Treatment before processing {len(df[df[TREATMENT]>EXPOSURE_THRESHOLD])}\")\n",
    "df['iA9'] = pd.to_datetime(df['iA9']) \n",
    "# sort values by ID and date\n",
    "df = df.sort_values(by = ['Clientid', 'iA9'])\n",
    "# drop nans on dates of assesment\n",
    "df = df[df['iA9'].isna()==False]\n",
    "# drop duplicated values\n",
    "df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9dc8fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in df.columns: \n",
    "    if column == 'iA9':\n",
    "        pass\n",
    "    # if not date\n",
    "    else:\n",
    "        df[column]=pd.to_numeric(df[column],errors='coerce')\n",
    "        # convert non-numeric to nun\n",
    "        df[column] = df[column].fillna(9999)\n",
    "        # replace NaN with =\n",
    "        df[column] = df[column].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce73171",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.replace(9999,np.nan,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2caf22a3",
   "metadata": {},
   "source": [
    "## Create Discrete Outcome to Represent Hospitalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5fdba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['in5a'])\n",
    "to_discrete = lambda x: 1 if x >= 1 else 0\n",
    "df['in5a'] = df['in5a'].apply(to_discrete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12dc22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['in5a']#.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eed1ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get number of classes\n",
    "NUM_CLASSES = len(df[OUTCOME].dropna().unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5c38f8",
   "metadata": {},
   "source": [
    "## Part 2. Data Processing\n",
    "\n",
    "In this first part, we will carry out the data processsing. In particular, we would like to select the relevant covariates, make a pre-selection based on patient characteristics (e.g. whether or not contra-indications are present), select the relevant period of observation, impute missing *coviariate* values and, lastly, handle any multi-collinearity. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbdc92a",
   "metadata": {},
   "source": [
    "Next, we define the lists of covariates, clinical indications and contra-indications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62abdb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define contra indications\n",
    "contra_indications = {'in2m': [1,2,3], 'ij6c': [1],\n",
    "                                 'ij5b': [3,4], 'ij4': [3,4]}\n",
    "\n",
    "# list relevant confounders\n",
    "confounders = ['ih1_dicho', 'iA2', 'ia12a_dicho','ie3e_dicho', 'ih3_dicho'\n",
    "                          'ij6a', 'sAGE_cat', 'ij1', 'ij2a_dicho', 'ij2b_dicho', 'ij2c_dicho', 'ij2d_dicho', \n",
    "               'ii1a_dicho', 'ii1b_dicho', \n",
    "               'ii1c_dicho', 'ii1d_dicho', 'ii1h_dicho', 'ii1m_dicho', 'il7_dicho'\n",
    "              'ii1j_dicho', 'ii1k_dicho', 'ii1l_dicho', 'ij5a',]\n",
    "\n",
    "            \n",
    "# list of other relevant variables\n",
    "relevant_vars = ['iA9', 'Clientid', TREATMENT, OUTCOME]\n",
    "\n",
    "print(len(contra_indications))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9547c109",
   "metadata": {},
   "source": [
    "Please note that you can get the meaning of the icodes by running the ``encoding``-function below. You only need to change the name of the ``code`` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27880938",
   "metadata": {},
   "outputs": [],
   "source": [
    "code = 'ij5a'\n",
    "print(encoding[(code).lower()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a19605",
   "metadata": {},
   "source": [
    "### 2.a. Indications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbe33a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#not applicable for exercise model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0f77f4",
   "metadata": {},
   "source": [
    "### 2a. Contra-indications\n",
    "\n",
    "Next, the first thing we want to do is to drop patients that have a contra-indication for the treatment in questions. In the code below, we specify the contra-indications. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de53fd0",
   "metadata": {},
   "source": [
    "Finally, for some contra-indications we want to exclude based on particular values only. We can do this by running the script below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411f9b2a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#run this later in the code to prevent dropping contra-indications in other measurement than baseline measurement. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4137c386",
   "metadata": {},
   "source": [
    "Having processed the contra-indications, we can now select the necessary columns in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586b484e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run this later in the code as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e3137a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicated cols if necessary\n",
    "df = df.loc[:,[True if i == False else False for i in df.T.index.duplicated(keep='first')]] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1cdf9c",
   "metadata": {},
   "source": [
    "### 2.b. Baseline-Followup Selection\n",
    "\n",
    "Important to note here is that in this particular setup, our goal is to **estimate the outcome value at follow up from the covariates *and* outcome variable at baseline**. \n",
    "\n",
    "Below you find a schematic depiction of how we want to process the data. As you can see, there are two timelines:\n",
    "\n",
    "1. **Timeline 1**: Naturally processes from baseline to follow up over time, without any intervention in the meantme.\n",
    "2. **Timeline 2**: Same as before, but with the difference that not an intervention has take place at some point in time between baseline and follow up. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5c28d1",
   "metadata": {},
   "source": [
    "![alt text](TimelineModels.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f61916",
   "metadata": {},
   "source": [
    "Our goal is therefore to, *for each observation*, take the values (**pre-treatment** covariates, outcome on baseline $y_{t}$) of baseline, and the  values at follow up (outcome on follow up $y_{t+1}$) and store it in a convenient manner. Important to note is that as patient may have *several* of these baseline-followup recors where sometimes an intervention may have occured and sometimes not. We decided to do this to, again, safe as much data as possible. \n",
    "\n",
    "Let us begin by first counting the number of assesments. We do this, because it is not possible to include patients without followup. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6297ed64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get number of assesments\n",
    "counter = lambda x: len(df[df['Clientid']==x])\n",
    "\n",
    "# count number of items\n",
    "df['num_assesments'] = df['Clientid'].apply(counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d2d9af",
   "metadata": {},
   "source": [
    "Let us select only observations with more than 2 assesments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046a82e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get number of assesments higher than 1\n",
    "df = df[df['num_assesments']>=2]\n",
    "\n",
    "print(f'{len(df)} observations remaining.')\n",
    "print(f\"Treatment after selecting on number of assesments {len(df[df[TREATMENT]>=EXPOSURE_THRESHOLD])}\")\n",
    "print(f\"No treatment after selecting on number of assesments {len(df[df[TREATMENT]<EXPOSURE_THRESHOLD])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f30a38",
   "metadata": {},
   "source": [
    "In the code below, we want to process the data a bit further. First, we want to make sure that the date columns (``iA9``) is in the correct format. Then we want to order based on ID and date. Also, we want to drop missing dates (if there are any) and drop any duplicated observations. Lastly, we want to make sure that the outcome is in the right format (i.e. a ``float``)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f4be75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# date already formatted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43866522",
   "metadata": {},
   "source": [
    "### 2.c Drop Duplicated Values\n",
    "\n",
    "Next, because there may be two assesments on the same date, we also want to drop those (regardless of the reason as inspecting that is beyond the scope of this project notebook). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d2ab96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'Clientid' and then apply the duplicate check on 'iA9' within each group\n",
    "duplicated_indices = df.groupby('Clientid').apply(\n",
    "    lambda x: x[x['iA9'].duplicated()].index).explode()\n",
    "\n",
    "# Drop NaN values from the index list if they exist\n",
    "duplicated_indices = duplicated_indices.dropna()\n",
    "\n",
    "# Convert the result into a list if it's not empty\n",
    "if not duplicated_indices.empty:\n",
    "    duplicated_indices = duplicated_indices.tolist()\n",
    "\n",
    "    # Drop the duplicates using the list of indices\n",
    "    df = df.drop(index=duplicated_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac32c7d6",
   "metadata": {},
   "source": [
    "### 2.d. Clean Compare\n",
    "If we want to only compare between those patients that receive either 0 treatment or a treatment above the threshold value, we run this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68930d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if CLEAN_COMPARE:\n",
    "    df = df[(df[TREATMENT] <= 0) | (df[TREATMENT] >= EXPOSURE_THRESHOLD)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221ddb34",
   "metadata": {},
   "source": [
    "### 2.e Period Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0a67a3",
   "metadata": {},
   "source": [
    "The piece of code (i.e. the function ``period_decomposition``) below is a loop that basically does the following:\n",
    "\n",
    "1. Create a temporary dataframe ``temp_df``. We do this to make sure that there is an empty entity (i.e. data storage) to which we can append new, cleaned and processed observations.\n",
    "2. Then, for each patient we do the following:\n",
    "3. Look at all the observation dates.\n",
    "4. Select the target outcome at follow up and at baseline.\n",
    "5. Append each period to the ``temp_df`` *until running out of dates*. \n",
    "6. Move to the next patient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e300f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def period_decomposition(df, target):\n",
    "    # List to store the rows of the new DataFrame\n",
    "    rows = []\n",
    "\n",
    "    # Unique client IDs\n",
    "    client_ids = df['Clientid'].unique()\n",
    "    total = len(client_ids)\n",
    "\n",
    "    for count, client_id in enumerate(client_ids, start=1):\n",
    "        # Inform the user about the progress\n",
    "        if count % 1000 == 0 or count == total:\n",
    "            print(f'{count} of {total} items completed...')\n",
    "\n",
    "        # Extract rows for the current client\n",
    "        client_rows = df[df['Clientid'] == client_id].sort_values('iA9')\n",
    "        client_dates = client_rows['iA9'].tolist()\n",
    "\n",
    "        for i in range(len(client_dates) - 1):\n",
    "            baseline_date = client_dates[i]\n",
    "            followup_date = client_dates[i + 1]\n",
    "\n",
    "            # Extract the rows for baseline and follow-up\n",
    "            baseline_row = client_rows[client_rows['iA9'] == baseline_date]\n",
    "            followup_row = client_rows[client_rows['iA9'] == followup_date]\n",
    "\n",
    "            # Calculate the outcomes\n",
    "            outcome_t0 = float(baseline_row[target])\n",
    "            outcome_t1 = float(followup_row[target])\n",
    "\n",
    "            # Prepare a new row with all necessary information\n",
    "            new_row = baseline_row.iloc[0].to_dict()\n",
    "            new_row['OutcomeT0'] = outcome_t0\n",
    "            new_row['OutcomeT1'] = outcome_t1\n",
    "            new_row['OutcomeT0Date'] = baseline_date\n",
    "            new_row['OutcomeT1Date'] = followup_date\n",
    "            rows.append(new_row)\n",
    "\n",
    "    # Create a new DataFrame from the list of new rows\n",
    "    temp_df = pd.DataFrame(rows)\n",
    "    print(\"Completed.\")\n",
    "    return temp_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa46020",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = period_decomposition(df, target = OUTCOME)\n",
    "print(f\"Treatment after processing {len(df[df[TREATMENT]>EXPOSURE_THRESHOLD])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fad135c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now run contra-indications\n",
    "# initialize empty tracker\n",
    "# iterate over columns and corresponding values \n",
    "for key, value in contra_indications.items():    \n",
    "    # loop over items in list\n",
    "    for i in value:\n",
    "        # drop values\n",
    "        df = df[df[key]!=i]\n",
    "        print(f'{i}-valued items from {encoding[key.lower()]} dropped.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113b7eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now select all relevant features\n",
    "\n",
    "df = df[confounders + relevant_vars + list(contra_indications.keys())]\n",
    "print(f\"Treatment after selecting covariates {len(df[df[TREATMENT]>=EXPOSURE_THRESHOLD])}\")\n",
    "print(f\"No treatment after selecting covariates {len(df[df[TREATMENT]<EXPOSURE_THRESHOLD])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2ffe04",
   "metadata": {},
   "source": [
    "### 2.f Dichotomize Treatment Variable\n",
    "\n",
    "Because our model can only deal with binary treatments, we need to convert the continuous treatments into binary ones. We will do that using a simple fuctions. Recall that the threshold has been set using the ``EXPOSURE_THRESHOLD`` variable above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baea95fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary = lambda x: 1 if x >= EXPOSURE_THRESHOLD else 0\n",
    "# convert treatment to binary\n",
    "df['treatment'] = df[TREATMENT].apply(binary)  \n",
    "df = df.drop(columns = [TREATMENT])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5068ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows with missing outcome or treatment \n",
    "df = df.dropna(subset = ['OutcomeT0', 'OutcomeT1', 'treatment'])\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ffd723",
   "metadata": {},
   "source": [
    "### 2.g Select Relevant Period\n",
    "\n",
    "Because the base data is too crude (the time periods between follow-up and baseline may be too far apart), we want to select a suitable period between observations. In essence, we aim to select a time window which is, on the one hand, determined by the minimum time (``PERIOD_MIN``) between baseline measurement and follow-up measurement and the maximum of that time window (``PERIOD_MAX``)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2feba781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get difference between dates\n",
    "df['date_diff'] = df['OutcomeT1Date'] - df['OutcomeT0Date'] \n",
    "df['date_diff'] = df['date_diff'].dt.days \n",
    "\n",
    "# select relevant period\n",
    "df = df[df['date_diff'] <= PERIOD_MAX]\n",
    "df = df[df['date_diff'] >= PERIOD_MIN]\n",
    "\n",
    "df = df.drop(columns = ['OutcomeT1Date', 'OutcomeT0Date', 'date_diff', 'iA9', OUTCOME, 'Clientid'])\n",
    "# examine how many treatment observations are left\n",
    "len(df[df['treatment']==1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479d5175",
   "metadata": {},
   "source": [
    "### 2.h Impute Missing Values\n",
    "\n",
    "In this part, we impute missing values. We use a standard KNN-imputer. If desired, it is possible to change the number of neighbours. Keep in mind that we can set this to ``False`` if we do not want to run it,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fea014",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imputing = IMPUTE does not impute outcome and treatment but all other included features\n",
    "if IMPUTE:\n",
    "    df = impute_missing_values_knn(df, n_neighbors=5)\n",
    "else:\n",
    "    df = df.dropna()\n",
    "    print(len(df[df['treatment']==1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a768ca",
   "metadata": {},
   "source": [
    "### 2.i Handle Multicollinearity\n",
    "\n",
    "Here we will handle multi-collinear columns. Keep in mind that we can set this to ``False`` if we do not want to run it,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7767c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this only applies for categorical outcomes! 'cor' in pandas runs a pearson correlation. \n",
    "if REMOVE_MULTI_COL:\n",
    "    df = multicol(df, CORRELATION_THRESHOLD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c01027c",
   "metadata": {},
   "source": [
    "### 2.j Handle Empty Columns\n",
    "\n",
    "In the process, it may be that we end up with columns that contain only $0$ values. We want to drop these columns, as they are not informative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6579421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop num_assesment column, not relevant anymore\n",
    "df = df.drop(columns = ['num_assesments'])\n",
    "# get a list of coviariate columns\n",
    "X = df.drop(columns = ['OutcomeT1', 'OutcomeT0', 'treatment']).columns\n",
    "\n",
    "# drop columns that only have 0 values\n",
    "for col in X:\n",
    "    if len(df[df[col]==0]) == len(df):\n",
    "        print(f\"{col} - {encoding[col.lower()]} dropped.\" )\n",
    "        df = df.drop(columns = [col])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391772f0",
   "metadata": {},
   "source": [
    "### 2.k Store CSV\n",
    "\n",
    "In the last part of this notebook, we will store that data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f99ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns = ['OutcomeT1', 'OutcomeT0', 'treatment']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b6e9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(f\"data/26-03-2024-Dutch_HC_cleaned_data_with_selected_covar_V2_{TREATMENT}-{OUTCOME}.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91c1d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD SKIP FOR TREATMENT AND OUTCOME T1\n",
    "covars = [str(i) for i in df.columns]\n",
    "covar_names = []\n",
    "\n",
    "#print(encoding[(code).lower()])\n",
    "\n",
    "for covar in covars:\n",
    "    if (covar != 'treatment') and (covar != 'OutcomeT1'):\n",
    "\n",
    "        try:\n",
    "            covar_names.append(encoding[(covar).lower()])\n",
    "        except:\n",
    "            covar_names.append(covar)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d9e876",
   "metadata": {},
   "outputs": [],
   "source": [
    "covar_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32729f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD SKIP FOR TREATMENT AND OUTCOME T1\n",
    "covars = [str(i) for i in df.columns]\n",
    "covar_names = []\n",
    "\n",
    "#print(encoding[(code).lower()])\n",
    "\n",
    "for covar in covars:\n",
    "    if (covar != 'treatment') and (covar != 'OutcomeT1'):\n",
    "\n",
    "        try:\n",
    "            covar_names.append(covar)\n",
    "        except:\n",
    "            covar_names.append(covar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21dd39d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "covar_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63130f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run feature importances\n",
    "# quick model\n",
    "clf = RandomForestClassifier()\n",
    "\n",
    "# get data\n",
    "X = df.drop(columns = ['OutcomeT1'])\n",
    "y = df['OutcomeT1']\n",
    "\n",
    "# train model\n",
    "clf.fit(X, y)\n",
    "importances = clf.feature_importances_\n",
    "feature_names = [] #X.columns\n",
    "\n",
    "# get col names\n",
    "for col in X.columns:\n",
    "    try:\n",
    "        feature_names.append(encoding[(col).lower()])\n",
    "    except:\n",
    "        feature_names.append(col)\n",
    "\n",
    "# get feature importance\n",
    "feature_importances = pd.DataFrame({'feature': feature_names, 'importance': importances})\n",
    "feature_importances = feature_importances.sort_values(by = 'importance', ascending = False)\n",
    "\n",
    "feature_importances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e28108",
   "metadata": {},
   "source": [
    "### Note to Teams: Please Start Here\n",
    "\n",
    "**MAKE SURE THAT THE DATA IS IN THE EXACT FORMAT AS BELOW.**\n",
    "\n",
    "Checklist:\n",
    "\n",
    "1. The covariates must come first.\n",
    "2. Then, there must be the outcome at baseline called ``OutcomeT0``.\n",
    "3. There must be an outcome at followup called ``OutcomeT1``.\n",
    "4. You may have a propensity score columns (``ps``), but this is not required. Same for clusters.\n",
    "5. There must be a treatment column *at the end* called ``treatment``. \n",
    "\n",
    "Good luck!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d09e493",
   "metadata": {},
   "source": [
    "## Part 3. Machine Learning\n",
    "\n",
    "Having processed the data, we can now move on the the machine learning (ML) part. Recall that we model using the meta-learning paradigm. Below is a schematic depiction of the process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21542a9",
   "metadata": {},
   "source": [
    "![alt text](images\\ModelGoal.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5e0980",
   "metadata": {},
   "source": [
    "As can be seen, for a given patient, we can - *after training* - provide the details of that patient. These details are the outcome at baseline (``OutcomeT0``), the covariates and whether or not we include treatment. This allows us to compute the **individual treatment effect** (ITE), the predicted effect unter treatment minus the predicted effect under control.\n",
    "\n",
    "However, before we are able to estimate the ITE for a patient, we need to train the model. In meta-learning we always train two models. The first model is trained on the control group and the second model is trained on the treated group. Let us now first state what the ``outcome`` and ``intervention`` variables are.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a271a03a",
   "metadata": {},
   "source": [
    "![alt text](images\\GeneralPlotTrajectoryInterpretation.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4a4fab",
   "metadata": {},
   "source": [
    "Next, we want to select the models. Here we will use ``RandomForestClassifier()``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0f03e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model0 = RandomForestClassifier()\n",
    "model1 = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42b2079",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e795bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File path for the Excel file\n",
    "excel_file = f'data/hyperparameters/model_hyperparameters_RandomForest.xlsx'\n",
    "\n",
    "# Create a Pandas Excel writer using XlsxWriter as the engine\n",
    "with pd.ExcelWriter(excel_file, engine='xlsxwriter') as writer:\n",
    "    # Get hyperparameters\n",
    "    hyperparameters = model.get_params()\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame([hyperparameters])\n",
    "\n",
    "    # Write each model's hyperparameters to a different sheet\n",
    "    df.to_excel(writer, sheet_name=\"RandomForest\")\n",
    "\n",
    "print(\"Hyperparameters of models have been saved to 'model_hyperparameters.xlsx'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5363a98",
   "metadata": {},
   "source": [
    "Then, we want to retrieve the data. We can use the data from above, but here we will read the file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45337bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# call your data file here\n",
    "FILE = f\"data/26-03-2024-Dutch_HC_cleaned_data_with_selected_covar_V2_{TREATMENT}-{OUTCOME}.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ac62fe",
   "metadata": {},
   "source": [
    "Afterwards, we can run the model. The only thing to do is to run the ``run_model``-function below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87d1784",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics, predictions_t, predictions_c, y_test_t, y_test_c, ites_test, ites_train, X_test_t, X_test_c, X_train_c, X_train_t, ites_test_proba = run_model_class(n_bootstraps = 100, file = FILE, model0 = model0, model1 = model1, undersampled = UNDERSAMPLE, intervention = TREATMENT, include_propensity = PROPENSITY, outcome = OUTCOME, machine = \"RandomForest\", num_iter = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59144fd5",
   "metadata": {},
   "source": [
    "We can now examine the performance of the model by calling ``metrics``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23472f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9644e60d",
   "metadata": {},
   "source": [
    "We can also visualise the model's *factual* predicted performance by calling the ``visualise`` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c67a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "machine = \"RandomForest\"\n",
    "# visualise(predictions_t, predictions_c, y_test_t, y_test_c, machine=machine, target=OUTCOME, intervention=TREATMENT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8c4e25",
   "metadata": {},
   "source": [
    "Also, if we want to visualize the ITES, we can run the following function (``visualise_ites``):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54274955",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualise_ites(ites_test, machine = machine, intervention = TREATMENT, target = OUTCOME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5367029e",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualise_ites(ites_train, machine = machine, intervention = TREATMENT, target = OUTCOME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76538fdb",
   "metadata": {},
   "source": [
    "### 3.b Run More Models\n",
    "\n",
    "These models can be run also. Feel free to remove some of the models, but please run at least:\n",
    "\n",
    "- ``RandomForestClassifier()``\n",
    "- ``MLPClassifier()``\n",
    "- ``LogisticRegression()``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b530bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_lst = [(RandomForestClassifier(),RandomForestClassifier()), \n",
    "            (MLPClassifier(), MLPClassifier()), (DecisionTreeClassifier(), DecisionTreeClassifier()),\n",
    "             (AdaBoostClassifier(), AdaBoostClassifier()), (GradientBoostingClassifier(), GradientBoostingClassifier()),\n",
    "             (ExtraTreesClassifier(),ExtraTreesClassifier()),(LogisticRegression(), LogisticRegression())]\n",
    "\n",
    "for model1, model2 in model_lst:\n",
    "    print(str(model1))\n",
    "    metrics, predictions_t, predictions_c, y_test_t, y_test_c, ites_test, ites_train, X_test_t, X_test_c, X_train_c, X_train_t, ites_test_proba = run_model_class(n_bootstraps = 100, file = FILE, intervention = TREATMENT, outcome = OUTCOME, include_propensity = PROPENSITY, undersampled = UNDERSAMPLE, model0 = model0, model1 = model1, machine = str(model1), num_iter = 1)\n",
    "    #visualise(predictions_t, predictions_c, y_test_t, y_test_c, machine=machine, target=target, intervention=intervention)\n",
    "    visualise_ites(ites_test, machine = str(model1), intervention = TREATMENT, target = OUTCOME)\n",
    "    #print(\"Probabilitiy of Hospitalization: \")\n",
    "    visualise_ites_proba(ites_test_proba, machine = machine, intervention = TREATMENT, target = OUTCOME)\n",
    "    print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e356b6",
   "metadata": {},
   "source": [
    "## Part 4. Adaptive Model\n",
    "\n",
    "In this last part, we will predict the treatment effect using an adaptive model. This model was developed by us. As can be seen, there is a bit more coding involved, but the benefit is that it is easier to adapt this model to our preferences. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1d8a6e",
   "metadata": {},
   "source": [
    "The first step is to make sure that our model runs on the proper device. \n",
    "\n",
    "Not included. Does not work with probabilistic outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21862932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# not appliable in classification model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
